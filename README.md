🚀 Analyzing 54M E-Commerce Transactions: Python, Big Data & Green Coding 🌍

"What if optimizing Python code could reduce a data team’s carbon footprint while unlocking business insights?"
I’m excited to share a project merging technical scalability with environmental responsibility:

📊 Project: Customer Retention Strategy for Online Retail (Scaled to 54M Rows)
→ Dataset: UCI Online Retail 2015
→ Goal: Predict churn risks and boost customer loyalty using transactional patterns.

🔍 Key Workflow:
✔️ RFM Analysis (Recency/Frequency/Monetary):

Identified high-risk clients (e.g., Recency > 6 months = 23% churn likelihood).

VIP segmentation: 18% of clients driving 62% of revenue.
✔️ Data Engineering: Cleaned 32M+ rows (handling duplicates, NaN, outliers).
✔️ Scalability Test: Pushed FireDucks to process ×60 scaled data (54M rows) seamlessly.

⚡ Technical Wins:
→ FireDucks vs Pandas:

2x faster execution 🚀 (see benchmarks below).

50% lower memory usage → Reduced cloud compute costs and CO₂ emissions.
→ Stack: Google Colab Pro + TPUv2-8 for heavy lifting.

📈 Business Impact:

Designed targeted reactivation campaigns for at-risk clients.

Optimized inventory based on regional return rates.

Automated report generation for stakeholder dashboards.

💡 Why This Matters:
→ Proves scalable analytics don’t require expensive infrastructure.
→ Green coding isn’t a buzzword—it’s measurable (small optimizations = big eco-impact!).


👉 Let’s connect if you’re tackling Big Data challenges!
📩 Open to collabs • 💬 Comments welcome • 🌱 Passionate about Green Tech

Check the code: https://github.com/lionel-richy/Large-Scale_Datenanalyse/

#Lionel Panlap #BigData #Python #DataScience #Sustainability #CustomerAnalytics #Open to work
